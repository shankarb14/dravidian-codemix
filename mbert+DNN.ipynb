{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mbert+DNN",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkHS68fkzNft"
      },
      "source": [
        "import subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "df = pd.read_csv('/content/hate_speech.tsv', sep='\\t').fillna(' ') #read_excel('/content/train.xlsx')\n",
        "print(df.head())\n",
        "# Create a list of punctuation marks\n",
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
        " '•', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']\n",
        "\n",
        "# Code to replace punctuations with whitespaces\n",
        "def clean_text(x):\n",
        "    x = str(x)\n",
        "    for punct in puncts:\n",
        "        if punct in x:\n",
        "            x = x.replace(punct, ' ')\n",
        "    return x\n",
        "df['Cleaned'] = df['Text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
        "df['Cleaned'] = df['Cleaned'].apply(lambda x: re.sub(\"@[\\w]*\", '', x))\n",
        "df['Cleaned'] = df['Cleaned'].apply(lambda x: clean_text(x))\n",
        "df['Cleaned'] = df['Cleaned'].str.lower()\n",
        "df['Cleaned'] = df['Cleaned'].apply(lambda x:' '.join(x.split()))\n",
        "df['Sentiment'] = df['Label']\n",
        "df = df.drop(['Text','Label'],axis=1)\n",
        "print(df.head())\n",
        "!pip install transformers\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch import IntTensor\n",
        "import transformers as ppb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model = TFBertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "tokenized = batch['Cleaned'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True,max_length=40,truncation=True)))\n",
        "max_len = 0\n",
        "for i in tokenized.values:\n",
        "  #print(len(i))\n",
        "  if len(i) > max_len:\n",
        "    max_len = len(i)\n",
        "\n",
        "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
        "\n",
        "print(padded)\n",
        "output = model(padded)\n",
        "embedding=output.last_hidden_state\n",
        "features = embedding[:,0,:].numpy()\n",
        "bert2=bert[0:6000]\n",
        "df = pd.read_csv('/content/train (1).csv') \n",
        "y = df['label']\n",
        "\n",
        "y = np.array(list(map(lambda x: 1 if x==\"real\" else 0, y)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGeMRv-uzY85"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "berttrain_features, berttest_features, berttrain_labels, berttest_labels = train_test_split(features, y,  test_size=0.3,random_state=50)\n",
        "input_bert = Input(shape=(768,))\n",
        "dense_text = Dense(1000,activation='relu',kernel_regularizer=regularizers.l2(0.01), kernel_initializer=initializers.he_normal(seed=0))(input_bert)\n",
        "#dense_text = Dropout(0.4)(dense_text)\n",
        "dense_text = Dense(500,activation='relu',kernel_regularizer=regularizers.l2(0.01), kernel_initializer=initializers.he_normal(seed=0))(dense_text)\n",
        "dense_text = Dropout(0.4)(dense_text)\n",
        "dense_text = Dense(100,activation='relu',kernel_regularizer=regularizers.l2(0.01), kernel_initializer=initializers.he_normal(seed=0))(dense_text)\n",
        "dense_text = BatchNormalization()(dense_text)\n",
        "dense_bert_drop = Dropout(0.4)(dense_text)\n",
        "output = Dense(1, activation='sigmoid')(dense_bert_drop)\n",
        "\n",
        "model = Model(inputs=[input_bert,input_elmo], outputs=output)\n",
        "adam = optimizers.Adam(lr=1e-4)\n",
        "checkpoint = ModelCheckpoint(filepath='../checkpoints_gossip/dense_MM_model.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "#sgd = optimizers.SGD(lr=1e-4, clipnorm=1.)\n",
        "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf7IhXk1GjzU"
      },
      "source": [
        "history = model.fit([berttrain_features,elmotrain_features],berttrain_labels,batch_size =32,epochs=100)\n",
        "score = model.evaluate([berttest_features,elmotest_features] ,berttest_labels, verbose=1)\n",
        "print(\"Test Score:\", score[0])\n",
        "print(\"Test Accuracy:\", score[1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkV70HIK2DlX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOZ6SO1iASyY"
      },
      "source": [
        ""
      ]
    }
  ]
}